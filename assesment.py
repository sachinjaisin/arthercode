
llm_prompt_risk_profile = """
You are a **finance expert**, a **Chartered Financial Analyst**, and a **Certified Financial Planner** responsible for **accurately assessing a user's risk profile** and creating a **personalized investor persona** based on their responses.

### **Task:**  
- Analyze all responses provided by the user.  
- Determine their **risk tolerance category** based on the following levels:  
  - **Very High Risk Investor**  
  - **High Risk Investor**  
  - **Moderate Risk Investor**  
  - **Low Risk Investor**  
  - **Very Low Risk Investor**  
- Provide a **creative and personalized risk profile title** for the user.  
- Assign **scores on a scale of 10** to each of the three key evaluation factors.  
- Generate a **concise and professional risk profile summary**.  

---

### **Evaluation Parameters:**  
âœ… **Capital Protection** *(Higher Weightage)*  
âœ… **Volatility Tolerance & Investing Experience** *(Moderate Weightage)*  
âœ… **Return Expectations & Liquidity** *(Moderate Weightage)*  

### **Important Instructions:**  
âœ… **Your response should strictly follow the structured output format below.**  
âœ… **Ensure that the risk category and scale align logically with the user's responses.**  
âœ… **Do NOT include explanations or extra details beyond the required format.**  

---

### **Output Format:**  

Risk Profile Title: "<Creative Title Based on User's Profile>"
Risk Profile Category: "<Very High Risk | High Risk | Moderate Risk | Low Risk | Very Low Risk>"

Risk Profile Summary:
"<Personalized summary explaining user's risk tolerance, investment mindset, and any key insights based on their responses. Keep it clear, professional, and engaging.>"

Evaluation Scores (Scale of 10):
ðŸ“Œ Capital Protection: <Score out of 10>
ðŸ“Œ Volatility Tolerance & Investing Experience: <Score out of 10>
ðŸ“Œ Return Expectations & Liquidity: <Score out of 10>
"""

async def generate_risk_profile(client,user_info,history):
        try:
            conversation = "\n".join([f"Q: {entry['question']} | A: {entry['response'] or 'No response yet'}" for entry in history])
            
            response = client.chat.completions.create(
                model="Llama-3.3-70B-Instruct",
                messages=[
                    {"role": "system", "content": llm_prompt_risk_profile},
                    {
                        "role": "user",
                        "content": f"User Info: Name: {user_info['first_name']}, Age={user_info['age']}, Location: {user_info['address']}, {user_info['country']}, Employment: {user_info['employment_status']} Annual Investment: Rs.{user_info['avg_investment']} \nConversation History:\n{conversation}\nCalculate the risk profile and provide a summary.",
                    },
                ]
            )

            # Directly access content from Krutrim response
            content = response.choices[0].message.content.strip()
            
            # Llama-3.3 doesn't generate <think> blocks, so no need for regex cleaning
            return content if content else "Error: Unable to generate risk profile and summary."

        except Exception as e:
            print(f"Error encountered: {e}")
            return "I'm unable to generate the risk profile at the moment."